{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Data from the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `requests` library to make API requests, and the [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) library to parse the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by making [an API request](https://www.cloudflare.com/learning/security/api/what-is-api-call/) to https://icanhazdadjoke.com/, a site that provides random dad jokes. The documentation for the API is at https://icanhazdadjoke.com/api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>joke</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RCQKuHJBdib</td>\n",
       "      <td>I been watching a channel on TV that is strict...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               joke  status\n",
       "0  RCQKuHJBdib  I been watching a channel on TV that is strict...     200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a request to the website\n",
    "res = requests.get(\"https://icanhazdadjoke.com/\", headers={\"Accept\": \"application/json\"})\n",
    "\n",
    "# convert the response to json\n",
    "res_json = res.json()\n",
    "\n",
    "# add the joke to a dataframe\n",
    "df = pd.DataFrame([res_json])\n",
    "\n",
    "# print the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use Beautiful Soup to parse the HTML from a web page. Let's try scraping the [Wikipedia page for the Python programming language](https://en.wikipedia.org/wiki/Python_(programming_language))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Main page</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Main_Page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Contents</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Contents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Current events</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Portal:Current_e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random article</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Special:Random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>About Wikipedia</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:About</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128</th>\n",
       "      <td>Articles with NKC identifiers</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Category:Article...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>Articles with SUDOC identifiers</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Category:Article...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>Articles with example Python (programming lang...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Category:Article...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>About Wikipedia</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:About</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>Disclaimers</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Genera...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1133 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  \\\n",
       "0                                             Main page   \n",
       "1                                              Contents   \n",
       "2                                        Current events   \n",
       "3                                        Random article   \n",
       "4                                       About Wikipedia   \n",
       "...                                                 ...   \n",
       "1128                      Articles with NKC identifiers   \n",
       "1129                    Articles with SUDOC identifiers   \n",
       "1130  Articles with example Python (programming lang...   \n",
       "1131                                    About Wikipedia   \n",
       "1132                                        Disclaimers   \n",
       "\n",
       "                                                    URL  \n",
       "0               https://en.wikipedia.org/wiki/Main_Page  \n",
       "1      https://en.wikipedia.org/wiki/Wikipedia:Contents  \n",
       "2     https://en.wikipedia.org/wiki/Portal:Current_e...  \n",
       "3          https://en.wikipedia.org/wiki/Special:Random  \n",
       "4         https://en.wikipedia.org/wiki/Wikipedia:About  \n",
       "...                                                 ...  \n",
       "1128  https://en.wikipedia.org/wiki/Category:Article...  \n",
       "1129  https://en.wikipedia.org/wiki/Category:Article...  \n",
       "1130  https://en.wikipedia.org/wiki/Category:Article...  \n",
       "1131      https://en.wikipedia.org/wiki/Wikipedia:About  \n",
       "1132  https://en.wikipedia.org/wiki/Wikipedia:Genera...  \n",
       "\n",
       "[1133 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape the Wikipedia page\n",
    "\n",
    "# use requests to get the page html\n",
    "res = requests.get('https://en.wikipedia.org/wiki/Python_(programming_language)')\n",
    "\n",
    "# use beautiful soup to parse the html\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "# find all the links on the page\n",
    "links = soup.find_all('a', href=True)\n",
    "\n",
    "# extract urls and link texts\n",
    "link_data = []\n",
    "for link in links:\n",
    "    if link['href'].startswith('/wiki/') and not link['href'].startswith('/wiki/File:'):\n",
    "        link_data.append({'Text': link.get_text(), 'URL': 'https://en.wikipedia.org' + link['href']})\n",
    "\n",
    "# make a dataframe from the link data\n",
    "df = pd.DataFrame(link_data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more, check out the [documentation for requests](https://docs.python-requests.org/en/master/) and the [Beautiful Soup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
